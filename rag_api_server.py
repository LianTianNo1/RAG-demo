# -*- coding: utf-8 -*-
"""
RAG APIæœåŠ¡å™¨ - æä¾›æ ‡å‡†çš„Chat APIå’Œæ–‡ä»¶ä¸Šä¼ æ¥å£

@remarks è¿™æ˜¯ä¸€ä¸ªåŸºäºFastAPIçš„RAGæœåŠ¡ï¼Œæä¾›å…¼å®¹OpenAIæ ¼å¼çš„èŠå¤©æ¥å£ï¼Œ
         æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€æ™ºèƒ½RAGè§¦å‘ã€å·¥å…·è°ƒç”¨å±•ç¤ºç­‰åŠŸèƒ½
@author AI Assistant
@version 2.0
"""

import os
import json
import hashlib
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
from pathlib import Path

# FastAPIç›¸å…³å¯¼å…¥
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import asyncio

# æ–‡ä»¶ç›‘æ§
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# åŸæœ‰çš„RAGç³»ç»Ÿå¯¼å…¥
import pandas as pd
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# --- å…¨å±€é…ç½® ---
KNOWLEDGE_BASE_DIR = "./knowledge_base/"  # çŸ¥è¯†åº“æ–‡ä»¶å­˜å‚¨ç›®å½•
VECTOR_STORE_DIR = "./vector_store/"      # å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL_NAME = "qwen3:4b"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
API_HOST = "0.0.0.0"
API_PORT = 8000

# --- APIæ•°æ®æ¨¡å‹ ---
class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: user, assistant, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")

class ToolCall(BaseModel):
    """å·¥å…·è°ƒç”¨æ¨¡å‹"""
    id: str = Field(..., description="å·¥å…·è°ƒç”¨ID")
    type: str = Field(default="function", description="å·¥å…·ç±»å‹")
    function: Dict[str, Any] = Field(..., description="å‡½æ•°è°ƒç”¨ä¿¡æ¯")

class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹"""
    model: str = Field(default="rag-excel", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: float = Field(default=0.7, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(default=None, description="æœ€å¤§tokenæ•°")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼å“åº”")
    tools: Optional[List[Dict[str, Any]]] = Field(default=None, description="å¯ç”¨å·¥å…·åˆ—è¡¨")
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(default="auto", description="å·¥å…·é€‰æ‹©ç­–ç•¥")

class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”æ¨¡å‹"""
    id: str = Field(..., description="å“åº”ID")
    object: str = Field(default="chat.completion", description="å¯¹è±¡ç±»å‹")
    created: int = Field(..., description="åˆ›å»ºæ—¶é—´æˆ³")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    choices: List[Dict[str, Any]] = Field(..., description="å“åº”é€‰æ‹©åˆ—è¡¨")
    usage: Dict[str, int] = Field(..., description="ä½¿ç”¨ç»Ÿè®¡")

class FileUploadResponse(BaseModel):
    """æ–‡ä»¶ä¸Šä¼ å“åº”æ¨¡å‹"""
    success: bool = Field(..., description="ä¸Šä¼ æ˜¯å¦æˆåŠŸ")
    filename: str = Field(..., description="æ–‡ä»¶å")
    file_id: str = Field(..., description="æ–‡ä»¶ID")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    file_hash: str = Field(..., description="æ–‡ä»¶å“ˆå¸Œå€¼")

# --- å¢å¼ºçš„RAGç³»ç»Ÿ ---
class EnhancedRAGSystem:
    """
    å¢å¼ºçš„RAGç³»ç»Ÿï¼Œæ”¯æŒæ–‡ä»¶ç›‘æ§å’Œæ™ºèƒ½æ›´æ–°

    @remarks ç›¸æ¯”åŸç‰ˆå¢åŠ äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
             1. æ–‡ä»¶å˜åŒ–ç›‘æ§
             2. å‘é‡åº“ç¼“å­˜å’Œå¢é‡æ›´æ–°
             3. å·¥å…·è°ƒç”¨æ¨¡æ‹Ÿ
             4. æ–‡ä»¶çº§åˆ«çš„æŸ¥è¯¢æ”¯æŒ
    """

    def __init__(self, knowledge_base_dir: str, vector_store_dir: str,
                 embedding_model_name: str, llm_model_name: str):
        """
        åˆå§‹åŒ–å¢å¼ºRAGç³»ç»Ÿ

        @param knowledge_base_dir - çŸ¥è¯†åº“æ–‡ä»¶ç›®å½•
        @param vector_store_dir - å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•
        @param embedding_model_name - åµŒå…¥æ¨¡å‹åç§°
        @param llm_model_name - å¤§è¯­è¨€æ¨¡å‹åç§°
        """
        self.knowledge_base_dir = Path(knowledge_base_dir)
        self.vector_store_dir = Path(vector_store_dir)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.knowledge_base_dir.mkdir(exist_ok=True)
        self.vector_store_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–æ¨¡å‹
        print(f"æ­£åœ¨åˆå§‹åŒ–å¢å¼ºRAGç³»ç»Ÿ...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': 'cpu'}
        )
        self.llm = Ollama(model=llm_model_name)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False,
        )

        # å‘é‡æ•°æ®åº“å’Œæ–‡ä»¶å“ˆå¸Œç¼“å­˜
        self.vector_store = None
        self.file_hashes = {}  # æ–‡ä»¶å“ˆå¸Œç¼“å­˜ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜åŒ–
        self.last_update_time = None

        # åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._load_existing_vector_store()

        print(f"å¢å¼ºRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚")

    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼

        @param file_path - æ–‡ä»¶è·¯å¾„
        @returns æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        """
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™ {file_path}: {e}")
            return ""

    def _load_existing_vector_store(self):
        """
        åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

        @returns æ— è¿”å›å€¼
        """
        vector_store_path = self.vector_store_dir / "faiss_index"
        if vector_store_path.exists():
            try:
                print("æ­£åœ¨åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“...")
                self.vector_store = FAISS.load_local(
                    str(vector_store_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )

                # åŠ è½½æ–‡ä»¶å“ˆå¸Œç¼“å­˜
                hash_cache_path = self.vector_store_dir / "file_hashes.json"
                if hash_cache_path.exists():
                    with open(hash_cache_path, 'r', encoding='utf-8') as f:
                        self.file_hashes = json.load(f)

                print(f"æˆåŠŸåŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {self.vector_store.index.ntotal} ä¸ªå‘é‡ã€‚")
                self.last_update_time = datetime.now()
            except Exception as e:
                print(f"åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                self.vector_store = None

    def _save_vector_store(self):
        """
        ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜

        @returns æ— è¿”å›å€¼
        """
        if self.vector_store is not None:
            try:
                vector_store_path = self.vector_store_dir / "faiss_index"
                self.vector_store.save_local(str(vector_store_path))

                # ä¿å­˜æ–‡ä»¶å“ˆå¸Œç¼“å­˜
                hash_cache_path = self.vector_store_dir / "file_hashes.json"
                with open(hash_cache_path, 'w', encoding='utf-8') as f:
                    json.dump(self.file_hashes, f, ensure_ascii=False, indent=2)

                print("å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°ç£ç›˜ã€‚")
            except Exception as e:
                print(f"ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

    def check_files_changed(self) -> bool:
        """
        æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–

        @returns å¦‚æœæœ‰æ–‡ä»¶å˜åŒ–è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        current_hashes = {}
        files_changed = False

        # æ£€æŸ¥æ‰€æœ‰Excelæ–‡ä»¶
        for file_path in self.knowledge_base_dir.glob("*.xlsx"):
            current_hash = self._calculate_file_hash(file_path)
            file_key = str(file_path.relative_to(self.knowledge_base_dir))
            current_hashes[file_key] = current_hash

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ–‡ä»¶æˆ–æ–‡ä»¶å·²ä¿®æ”¹
            if file_key not in self.file_hashes or self.file_hashes[file_key] != current_hash:
                files_changed = True
                print(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {file_key}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è¢«åˆ é™¤
        for file_key in self.file_hashes:
            if file_key not in current_hashes:
                files_changed = True
                print(f"æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤: {file_key}")

        self.file_hashes = current_hashes
        return files_changed

    def _load_excel_documents(self) -> List[Document]:
        """
        ä»çŸ¥è¯†åº“ç›®å½•åŠ è½½æ‰€æœ‰Excelæ–‡ä»¶

        @returns Documentå¯¹è±¡åˆ—è¡¨
        """
        print(f"æ­£åœ¨ä»çŸ¥è¯†åº“ç›®å½•åŠ è½½Excelæ–‡ä»¶...")
        all_docs = []

        excel_files = list(self.knowledge_base_dir.glob("*.xlsx"))
        if not excel_files:
            print(f"è­¦å‘Šï¼šåœ¨çŸ¥è¯†åº“ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•Excelæ–‡ä»¶ã€‚")
            return []

        for file_path in excel_files:
            print(f"  æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path.name}")
            try:
                xls = pd.read_excel(file_path, sheet_name=None)
                for sheet_name, df in xls.items():
                    if not df.empty:
                        sheet_content = ""
                        for index, row in df.iterrows():
                            row_texts = []
                            for col_name, cell_value in row.items():
                                cell_text = str(cell_value).strip()
                                if cell_text and cell_text != 'nan':
                                    row_texts.append(f"{str(col_name).strip()}: {cell_text}")
                            if row_texts:
                                sheet_content += ", ".join(row_texts) + "\n"

                        if sheet_content.strip():
                            doc = Document(
                                page_content=sheet_content.strip(),
                                metadata={
                                    "source_file": file_path.name,
                                    "sheet_name": sheet_name,
                                    "file_path": str(file_path)
                                }
                            )
                            all_docs.append(doc)
            except Exception as e:
                print(f"    å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        print(f"Excelæ–‡ä»¶åŠ è½½å®Œæ¯•ï¼Œå…±åŠ è½½äº† {len(all_docs)} ä¸ªæ–‡æ¡£ã€‚")
        return all_docs

    def rebuild_vector_store(self) -> bool:
        """
        é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“

        @returns æ„å»ºæˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        print("æ­£åœ¨é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“...")

        # 1. åŠ è½½æ–‡æ¡£
        documents = self._load_excel_documents()
        if not documents:
            print("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“ã€‚")
            return False

        # 2. åˆ†å‰²æ–‡æ¡£
        text_chunks = self.text_splitter.split_documents(documents)
        if not text_chunks:
            print("æ–‡æ¡£åˆ†å‰²å¤±è´¥ï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“ã€‚")
            return False

        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±å¾—åˆ° {len(text_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

        # 3. æ„å»ºå‘é‡æ•°æ®åº“
        try:
            self.vector_store = FAISS.from_documents(documents=text_chunks, embedding=self.embeddings)
            self.last_update_time = datetime.now()

            # ä¿å­˜åˆ°ç£ç›˜
            self._save_vector_store()

            print(f"å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼ŒåŒ…å« {self.vector_store.index.ntotal} ä¸ªå‘é‡ã€‚")
            return True
        except Exception as e:
            print(f"æ„å»ºå‘é‡æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def update_if_needed(self) -> bool:
        """
        å¦‚æœæ–‡ä»¶æœ‰å˜åŒ–ï¼Œåˆ™æ›´æ–°å‘é‡æ•°æ®åº“

        @returns å¦‚æœè¿›è¡Œäº†æ›´æ–°è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if self.check_files_changed():
            print("æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–ï¼Œæ­£åœ¨æ›´æ–°å‘é‡æ•°æ®åº“...")
            return self.rebuild_vector_store()
        return False

    def query_with_tools(self, user_question: str, specific_files: Optional[List[str]] = None, k: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨å·¥å…·è¿›è¡ŒæŸ¥è¯¢ï¼Œè¿”å›åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯çš„ç»“æœ

        @param user_question - ç”¨æˆ·é—®é¢˜
        @param specific_files - æŒ‡å®šæŸ¥è¯¢çš„æ–‡ä»¶åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæŸ¥è¯¢æ‰€æœ‰æ–‡ä»¶
        @param k - æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        @returns åŒ…å«ç­”æ¡ˆå’Œå·¥å…·è°ƒç”¨ä¿¡æ¯çš„å­—å…¸
        """
        # ç¡®ä¿å‘é‡æ•°æ®åº“æ˜¯æœ€æ–°çš„
        self.update_if_needed()

        if self.vector_store is None:
            return {
                "answer": "é”™è¯¯ï¼šå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ã€‚è¯·å…ˆä¸Šä¼ ä¸€äº›Excelæ–‡ä»¶ã€‚",
                "tool_calls": [],
                "sources": []
            }

        # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
        tool_calls = []
        sources = []

        # 1. Excelæœç´¢å·¥å…·
        search_tool_call = {
            "id": f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "type": "function",
            "function": {
                "name": "excel_search",
                "arguments": {
                    "query": user_question,
                    "files": specific_files or "all",
                    "top_k": k
                }
            }
        }
        tool_calls.append(search_tool_call)

        try:
            # æ‰§è¡Œæ£€ç´¢
            if specific_files:
                # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œåˆ™è¿‡æ»¤æ£€ç´¢ç»“æœ
                retrieved_docs = self.vector_store.similarity_search(user_question, k=k*2)  # å¤šæ£€ç´¢ä¸€äº›ï¼Œç„¶åè¿‡æ»¤
                filtered_docs = []
                for doc in retrieved_docs:
                    if doc.metadata.get("source_file") in specific_files:
                        filtered_docs.append(doc)
                        if len(filtered_docs) >= k:
                            break
                retrieved_docs = filtered_docs
            else:
                retrieved_docs = self.vector_store.similarity_search(user_question, k=k)

            # æ”¶é›†æ¥æºä¿¡æ¯
            for doc in retrieved_docs:
                source_info = {
                    "file": doc.metadata.get("source_file", "unknown"),
                    "sheet": doc.metadata.get("sheet_name", "unknown"),
                    "content_preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                sources.append(source_info)

            if not retrieved_docs:
                context_text = "æœªåœ¨æŒ‡å®šçš„Excelæ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            else:
                context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

            # 2. LLMç”Ÿæˆå·¥å…·
            llm_tool_call = {
                "id": f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}_llm",
                "type": "function",
                "function": {
                    "name": "llm_generate",
                    "arguments": {
                        "context": context_text[:500] + "..." if len(context_text) > 500 else context_text,
                        "question": user_question,
                        "model": LLM_MODEL_NAME
                    }
                }
            }
            tool_calls.append(llm_tool_call)

            # æ„å»ºæç¤ºå¹¶ç”Ÿæˆç­”æ¡ˆ
            prompt_template = ChatPromptTemplate.from_template(
                """
                è¯·ä½ æ‰®æ¼”ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„èƒŒæ™¯ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œä¸è¦ç¼–é€ ã€‚
                è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚

                èƒŒæ™¯ä¿¡æ¯:
                {context}

                ç”¨æˆ·é—®é¢˜:
                {question}

                å›ç­”:
                """
            )

            rag_chain = prompt_template | self.llm | StrOutputParser()
            answer = rag_chain.invoke({"context": context_text, "question": user_question})

            return {
                "answer": answer,
                "tool_calls": tool_calls,
                "sources": sources
            }

        except Exception as e:
            error_msg = f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
            return {
                "answer": error_msg,
                "tool_calls": tool_calls,
                "sources": []
            }

    async def query_with_tools_stream(self, user_question: str, specific_files: Optional[List[str]] = None, k: int = 3):
        """
        ä½¿ç”¨å·¥å…·è¿›è¡Œæµå¼æŸ¥è¯¢ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨

        @param user_question - ç”¨æˆ·é—®é¢˜
        @param specific_files - æŒ‡å®šæŸ¥è¯¢çš„æ–‡ä»¶åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæŸ¥è¯¢æ‰€æœ‰æ–‡ä»¶
        @param k - æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        @returns å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œäº§ç”Ÿæµå¼å“åº”æ•°æ®
        """
        # ç¡®ä¿å‘é‡æ•°æ®åº“æ˜¯æœ€æ–°çš„
        self.update_if_needed()

        if self.vector_store is None:
            yield {
                "error": "å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ã€‚è¯·å…ˆä¸Šä¼ ä¸€äº›Excelæ–‡ä»¶ã€‚",
                "tool_calls": [],
                "sources": []
            }
            return

        # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
        tool_calls = []
        sources = []

        # 1. Excelæœç´¢å·¥å…·
        search_tool_call = {
            "id": f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "type": "function",
            "function": {
                "name": "excel_search",
                "arguments": {
                    "query": user_question,
                    "files": specific_files or "all",
                    "top_k": k
                }
            }
        }
        tool_calls.append(search_tool_call)

        # å…ˆå‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
        yield {
            "type": "tool_call",
            "tool_call": search_tool_call
        }

        try:
            # æ‰§è¡Œæ£€ç´¢
            if specific_files:
                retrieved_docs = self.vector_store.similarity_search(user_question, k=k*2)
                filtered_docs = []
                for doc in retrieved_docs:
                    if doc.metadata.get("source_file") in specific_files:
                        filtered_docs.append(doc)
                        if len(filtered_docs) >= k:
                            break
                retrieved_docs = filtered_docs
            else:
                retrieved_docs = self.vector_store.similarity_search(user_question, k=k)

            # æ”¶é›†æ¥æºä¿¡æ¯
            for doc in retrieved_docs:
                source_info = {
                    "file": doc.metadata.get("source_file", "unknown"),
                    "sheet": doc.metadata.get("sheet_name", "unknown"),
                    "content_preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                sources.append(source_info)

            # å‘é€æ£€ç´¢ç»“æœ
            yield {
                "type": "retrieval_result",
                "sources": sources,
                "retrieved_count": len(retrieved_docs)
            }

            if not retrieved_docs:
                context_text = "æœªåœ¨æŒ‡å®šçš„Excelæ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            else:
                context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

            # 2. LLMç”Ÿæˆå·¥å…·
            llm_tool_call = {
                "id": f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}_llm",
                "type": "function",
                "function": {
                    "name": "llm_generate",
                    "arguments": {
                        "context": context_text[:500] + "..." if len(context_text) > 500 else context_text,
                        "question": user_question,
                        "model": LLM_MODEL_NAME
                    }
                }
            }
            tool_calls.append(llm_tool_call)

            # å‘é€LLMå·¥å…·è°ƒç”¨
            yield {
                "type": "tool_call",
                "tool_call": llm_tool_call
            }

            # æ„å»ºæç¤ºå¹¶ç”Ÿæˆç­”æ¡ˆ
            prompt_template = ChatPromptTemplate.from_template(
                """
                è¯·ä½ æ‰®æ¼”ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„èƒŒæ™¯ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œä¸è¦ç¼–é€ ã€‚
                è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚

                èƒŒæ™¯ä¿¡æ¯:
                {context}

                ç”¨æˆ·é—®é¢˜:
                {question}

                å›ç­”:
                """
            )

            rag_chain = prompt_template | self.llm | StrOutputParser()

            # å¼€å§‹ç”Ÿæˆç­”æ¡ˆ
            yield {
                "type": "generation_start"
            }

            # æ¨¡æ‹Ÿæµå¼ç”Ÿæˆï¼ˆå› ä¸ºå½“å‰Ollamaä¸æ”¯æŒçœŸæ­£çš„æµå¼ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿåˆ†å—å‘é€ï¼‰
            answer = rag_chain.invoke({"context": context_text, "question": user_question})

            # å°†ç­”æ¡ˆåˆ†å—å‘é€
            words = answer.split()
            chunk_size = 3  # æ¯æ¬¡å‘é€3ä¸ªè¯

            for i in range(0, len(words), chunk_size):
                chunk = " ".join(words[i:i + chunk_size])
                if i + chunk_size < len(words):
                    chunk += " "

                yield {
                    "type": "content_chunk",
                    "content": chunk
                }

                # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
                await asyncio.sleep(0.1)

            # å‘é€å®Œæˆä¿¡æ¯
            yield {
                "type": "generation_complete",
                "full_answer": answer,
                "tool_calls": tool_calls,
                "sources": sources
            }

        except Exception as e:
            yield {
                "type": "error",
                "error": f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}",
                "tool_calls": tool_calls,
                "sources": []
            }

# --- å…¨å±€RAGç³»ç»Ÿå®ä¾‹ ---
rag_system = None

def get_rag_system() -> EnhancedRAGSystem:
    """
    è·å–å…¨å±€RAGç³»ç»Ÿå®ä¾‹

    @returns EnhancedRAGSystemå®ä¾‹
    """
    global rag_system
    if rag_system is None:
        rag_system = EnhancedRAGSystem(
            knowledge_base_dir=KNOWLEDGE_BASE_DIR,
            vector_store_dir=VECTOR_STORE_DIR,
            embedding_model_name=EMBEDDING_MODEL_NAME,
            llm_model_name=LLM_MODEL_NAME
        )
    return rag_system

# --- FastAPIåº”ç”¨åˆå§‹åŒ– ---
app = FastAPI(
    title="RAG Excel API",
    description="åŸºäºExcelæ–‡ä»¶çš„æ£€ç´¢å¢å¼ºç”ŸæˆAPIæœåŠ¡",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“çš„åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- APIç«¯ç‚¹ ---

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹ï¼Œè¿”å›APIä¿¡æ¯"""
    return {
        "message": "RAG Excel APIæœåŠ¡æ­£åœ¨è¿è¡Œ",
        "version": "2.0.0",
        "docs": "/docs",
        "web_demo": "/web_demo.html",
        "endpoints": {
            "chat": "/v1/chat/completions",
            "upload": "/v1/files/upload",
            "health": "/health"
        }
    }

@app.get("/web_demo.html")
async def web_demo():
    """æä¾›Webæ¼”ç¤ºç•Œé¢"""
    from fastapi.responses import FileResponse
    import os

    web_demo_path = os.path.join(os.path.dirname(__file__), "web_demo.html")
    if os.path.exists(web_demo_path):
        return FileResponse(web_demo_path, media_type="text/html")
    else:
        raise HTTPException(status_code=404, detail="Webæ¼”ç¤ºç•Œé¢æ–‡ä»¶æœªæ‰¾åˆ°")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    rag = get_rag_system()
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "vector_store_ready": rag.vector_store is not None,
        "last_update": rag.last_update_time.isoformat() if rag.last_update_time else None,
        "knowledge_base_files": len(list(Path(KNOWLEDGE_BASE_DIR).glob("*.xlsx")))
    }

@app.post("/v1/files/upload", response_model=FileUploadResponse)
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="è¦ä¸Šä¼ çš„Excelæ–‡ä»¶")
):
    """
    ä¸Šä¼ Excelæ–‡ä»¶åˆ°çŸ¥è¯†åº“

    @param file - ä¸Šä¼ çš„Excelæ–‡ä»¶
    @returns ä¸Šä¼ ç»“æœä¿¡æ¯
    """
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.endswith(('.xlsx', '.xls')):
        raise HTTPException(
            status_code=400,
            detail="åªæ”¯æŒExcelæ–‡ä»¶æ ¼å¼ (.xlsx, .xls)"
        )

    try:
        # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
        knowledge_base_path = Path(KNOWLEDGE_BASE_DIR)
        knowledge_base_path.mkdir(exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        file_path = knowledge_base_path / file.filename
        content = await file.read()

        with open(file_path, "wb") as f:
            f.write(content)

        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = hashlib.md5(content).hexdigest()

        # ç”Ÿæˆæ–‡ä»¶ID
        file_id = f"file_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_hash[:8]}"

        # åœ¨åå°ä»»åŠ¡ä¸­æ›´æ–°å‘é‡æ•°æ®åº“
        background_tasks.add_task(update_vector_store_background)

        return FileUploadResponse(
            success=True,
            filename=file.filename,
            file_id=file_id,
            message=f"æ–‡ä»¶ {file.filename} ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°æ›´æ–°çŸ¥è¯†åº“...",
            file_hash=file_hash
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        )

async def update_vector_store_background():
    """
    åå°ä»»åŠ¡ï¼šæ›´æ–°å‘é‡æ•°æ®åº“
    """
    try:
        rag = get_rag_system()
        rag.update_if_needed()
        print("åå°å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆã€‚")
    except Exception as e:
        print(f"åå°æ›´æ–°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    èŠå¤©å®Œæˆæ¥å£ï¼Œå…¼å®¹OpenAIæ ¼å¼ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”

    @param request - èŠå¤©è¯·æ±‚
    @returns èŠå¤©å“åº”æˆ–æµå¼å“åº”
    """
    # å¦‚æœè¯·æ±‚æµå¼å“åº”
    if request.stream:
        return StreamingResponse(
            generate_stream_response(request),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )

    # éæµå¼å“åº”ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break

        if not user_message:
            raise HTTPException(
                status_code=400,
                detail="æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯"
            )

        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†ç‰¹å®šæ–‡ä»¶ï¼ˆä»æ¶ˆæ¯ä¸­è§£æï¼‰
        specific_files = None
        # è¿™é‡Œå¯ä»¥æ·»åŠ è§£æé€»è¾‘ï¼Œæ¯”å¦‚æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å« "åœ¨æ–‡ä»¶Xä¸­" è¿™æ ·çš„æŒ‡ä»¤

        # ä½¿ç”¨RAGç³»ç»ŸæŸ¥è¯¢
        rag = get_rag_system()
        result = rag.query_with_tools(user_message, specific_files)

        # æ„å»ºå“åº”
        response_id = f"chatcmpl-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        created_timestamp = int(datetime.now().timestamp())

        # æ„å»ºå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        tool_calls_data = []
        if result.get("tool_calls"):
            for tool_call in result["tool_calls"]:
                tool_calls_data.append({
                    "id": tool_call["id"],
                    "type": tool_call["type"],
                    "function": {
                        "name": tool_call["function"]["name"],
                        "arguments": json.dumps(tool_call["function"]["arguments"], ensure_ascii=False)
                    }
                })

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        message_content = result["answer"]

        # å¦‚æœæœ‰æ¥æºä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯æœ«å°¾
        if result.get("sources"):
            message_content += "\n\nğŸ“š **ä¿¡æ¯æ¥æº:**\n"
            for i, source in enumerate(result["sources"], 1):
                message_content += f"{i}. æ–‡ä»¶: {source['file']}, å·¥ä½œè¡¨: {source['sheet']}\n"

        choice = {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": message_content,
                "tool_calls": tool_calls_data if tool_calls_data else None
            },
            "finish_reason": "stop"
        }

        # ç§»é™¤Noneå€¼
        if choice["message"]["tool_calls"] is None:
            del choice["message"]["tool_calls"]

        return ChatCompletionResponse(
            id=response_id,
            created=created_timestamp,
            model=request.model,
            choices=[choice],
            usage={
                "prompt_tokens": len(user_message.split()),  # ç®€å•ä¼°ç®—
                "completion_tokens": len(result["answer"].split()),  # ç®€å•ä¼°ç®—
                "total_tokens": len(user_message.split()) + len(result["answer"].split())
            }
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"èŠå¤©å¤„ç†å¤±è´¥: {str(e)}"
        )

async def generate_stream_response(request: ChatCompletionRequest):
    """
    ç”Ÿæˆæµå¼å“åº”çš„å¼‚æ­¥ç”Ÿæˆå™¨

    @param request - èŠå¤©è¯·æ±‚
    @returns å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œäº§ç”ŸSSEæ ¼å¼çš„æ•°æ®
    """
    try:
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break

        if not user_message:
            yield f"data: {json.dumps({'error': 'æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯'}, ensure_ascii=False)}\n\n"
            return

        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†ç‰¹å®šæ–‡ä»¶
        specific_files = None

        # ä½¿ç”¨RAGç³»ç»Ÿè¿›è¡Œæµå¼æŸ¥è¯¢
        rag = get_rag_system()

        response_id = f"chatcmpl-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        created_timestamp = int(datetime.now().timestamp())

        # å‘é€åˆå§‹å“åº”
        initial_response = {
            "id": response_id,
            "object": "chat.completion.chunk",
            "created": created_timestamp,
            "model": request.model,
            "choices": [{
                "index": 0,
                "delta": {"role": "assistant", "content": ""},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(initial_response, ensure_ascii=False)}\n\n"

        # æµå¼å¤„ç†
        full_content = ""
        tool_calls_data = []
        sources_data = []

        async for chunk in rag.query_with_tools_stream(user_message, specific_files):
            chunk_type = chunk.get("type")

            if chunk_type == "tool_call":
                # å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_call = chunk["tool_call"]
                tool_calls_data.append({
                    "id": tool_call["id"],
                    "type": tool_call["type"],
                    "function": {
                        "name": tool_call["function"]["name"],
                        "arguments": json.dumps(tool_call["function"]["arguments"], ensure_ascii=False)
                    }
                })

                # å‘é€å·¥å…·è°ƒç”¨å—
                tool_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "tool_calls": [{
                                "index": len(tool_calls_data) - 1,
                                "id": tool_call["id"],
                                "type": tool_call["type"],
                                "function": {
                                    "name": tool_call["function"]["name"],
                                    "arguments": json.dumps(tool_call["function"]["arguments"], ensure_ascii=False)
                                }
                            }]
                        },
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(tool_chunk, ensure_ascii=False)}\n\n"

            elif chunk_type == "retrieval_result":
                # ä¿å­˜æ¥æºä¿¡æ¯
                sources_data = chunk["sources"]

                # å‘é€æ£€ç´¢ç»“æœä¿¡æ¯
                retrieval_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": f"ğŸ” æ£€ç´¢åˆ° {chunk['retrieved_count']} ä¸ªç›¸å…³æ–‡æ¡£\n"
                        },
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(retrieval_chunk, ensure_ascii=False)}\n\n"

            elif chunk_type == "generation_start":
                # å‘é€ç”Ÿæˆå¼€å§‹ä¿¡æ¯
                start_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": "ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...\n\n"},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(start_chunk, ensure_ascii=False)}\n\n"

            elif chunk_type == "content_chunk":
                # å‘é€å†…å®¹å—
                content = chunk["content"]
                full_content += content

                content_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": content},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(content_chunk, ensure_ascii=False)}\n\n"

            elif chunk_type == "generation_complete":
                # æ·»åŠ æ¥æºä¿¡æ¯
                if sources_data:
                    sources_text = "\n\nğŸ“š **ä¿¡æ¯æ¥æº:**\n"
                    for i, source in enumerate(sources_data, 1):
                        sources_text += f"{i}. æ–‡ä»¶: {source['file']}, å·¥ä½œè¡¨: {source['sheet']}\n"

                    sources_chunk = {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": created_timestamp,
                        "model": request.model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": sources_text},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(sources_chunk, ensure_ascii=False)}\n\n"

                # å‘é€å®Œæˆå—
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"

            elif chunk_type == "error":
                # å‘é€é”™è¯¯ä¿¡æ¯
                error_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_timestamp,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": f"âŒ é”™è¯¯: {chunk['error']}"},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"

    except Exception as e:
        error_chunk = {
            "id": f"chatcmpl-error-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "object": "chat.completion.chunk",
            "created": int(datetime.now().timestamp()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "delta": {"content": f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

@app.get("/v1/files/list")
async def list_files():
    """
    åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶

    @returns æ–‡ä»¶åˆ—è¡¨ä¿¡æ¯
    """
    try:
        knowledge_base_path = Path(KNOWLEDGE_BASE_DIR)
        files = []

        for file_path in knowledge_base_path.glob("*.xlsx"):
            file_stat = file_path.stat()
            file_hash = ""
            try:
                with open(file_path, "rb") as f:
                    content = f.read()
                    file_hash = hashlib.md5(content).hexdigest()
            except Exception:
                pass

            files.append({
                "filename": file_path.name,
                "size": file_stat.st_size,
                "modified_time": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                "file_hash": file_hash
            })

        return {
            "files": files,
            "total_count": len(files),
            "knowledge_base_dir": str(knowledge_base_path.absolute())
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.delete("/v1/files/{filename}")
async def delete_file(filename: str, background_tasks: BackgroundTasks):
    """
    åˆ é™¤çŸ¥è¯†åº“ä¸­çš„æŒ‡å®šæ–‡ä»¶

    @param filename - è¦åˆ é™¤çš„æ–‡ä»¶å
    @returns åˆ é™¤ç»“æœ
    """
    try:
        file_path = Path(KNOWLEDGE_BASE_DIR) / filename

        if not file_path.exists():
            raise HTTPException(
                status_code=404,
                detail=f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨"
            )

        if not file_path.suffix.lower() in ['.xlsx', '.xls']:
            raise HTTPException(
                status_code=400,
                detail="åªèƒ½åˆ é™¤Excelæ–‡ä»¶"
            )

        # åˆ é™¤æ–‡ä»¶
        file_path.unlink()

        # åœ¨åå°æ›´æ–°å‘é‡æ•°æ®åº“
        background_tasks.add_task(update_vector_store_background)

        return {
            "success": True,
            "message": f"æ–‡ä»¶ {filename} åˆ é™¤æˆåŠŸï¼Œæ­£åœ¨åå°æ›´æ–°çŸ¥è¯†åº“...",
            "filename": filename
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"
        )

@app.post("/v1/vector_store/rebuild")
async def rebuild_vector_store(background_tasks: BackgroundTasks):
    """
    æ‰‹åŠ¨é‡å»ºå‘é‡æ•°æ®åº“

    @returns é‡å»ºä»»åŠ¡çŠ¶æ€
    """
    try:
        # åœ¨åå°ä»»åŠ¡ä¸­é‡å»ºå‘é‡æ•°æ®åº“
        background_tasks.add_task(rebuild_vector_store_background)

        return {
            "success": True,
            "message": "å‘é‡æ•°æ®åº“é‡å»ºä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·ç¨åæŸ¥çœ‹çŠ¶æ€",
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å¯åŠ¨é‡å»ºä»»åŠ¡å¤±è´¥: {str(e)}"
        )

async def rebuild_vector_store_background():
    """
    åå°ä»»åŠ¡ï¼šå¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“
    """
    try:
        rag = get_rag_system()
        success = rag.rebuild_vector_store()
        if success:
            print("åå°å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆã€‚")
        else:
            print("åå°å‘é‡æ•°æ®åº“é‡å»ºå¤±è´¥ã€‚")
    except Exception as e:
        print(f"åå°é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")

# --- å¯åŠ¨äº‹ä»¶ ---
@app.on_event("startup")
async def startup_event():
    """
    åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–äº‹ä»¶
    """
    print("ğŸš€ RAG Excel APIæœåŠ¡å¯åŠ¨ä¸­...")

    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Path(KNOWLEDGE_BASE_DIR).mkdir(exist_ok=True)
    Path(VECTOR_STORE_DIR).mkdir(exist_ok=True)

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = get_rag_system()

    # å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰æ–‡ä»¶ä½†æ²¡æœ‰å‘é‡æ•°æ®åº“ï¼Œåˆ™æ„å»º
    knowledge_files = list(Path(KNOWLEDGE_BASE_DIR).glob("*.xlsx"))
    if knowledge_files and rag.vector_store is None:
        print("æ£€æµ‹åˆ°çŸ¥è¯†åº“æ–‡ä»¶ä½†å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
        rag.rebuild_vector_store()

    print("âœ… RAG Excel APIæœåŠ¡å¯åŠ¨å®Œæˆï¼")
    print(f"ğŸ“ çŸ¥è¯†åº“ç›®å½•: {Path(KNOWLEDGE_BASE_DIR).absolute()}")
    print(f"ğŸ—„ï¸ å‘é‡åº“ç›®å½•: {Path(VECTOR_STORE_DIR).absolute()}")
    print(f"ğŸ“Š å½“å‰çŸ¥è¯†åº“æ–‡ä»¶æ•°: {len(knowledge_files)}")
    print(f"ğŸ”— APIæ–‡æ¡£: http://{API_HOST}:{API_PORT}/docs")

@app.on_event("shutdown")
async def shutdown_event():
    """
    åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†äº‹ä»¶
    """
    print("ğŸ›‘ RAG Excel APIæœåŠ¡æ­£åœ¨å…³é—­...")

    # ä¿å­˜å‘é‡æ•°æ®åº“
    rag = get_rag_system()
    if rag.vector_store is not None:
        rag._save_vector_store()
        print("ğŸ’¾ å‘é‡æ•°æ®åº“å·²ä¿å­˜")

    print("âœ… RAG Excel APIæœåŠ¡å·²å®‰å…¨å…³é—­")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    import uvicorn

    print("ğŸ¯ å¯åŠ¨RAG Excel APIæœåŠ¡å™¨...")
    print(f"ğŸ“ æœåŠ¡åœ°å€: http://{API_HOST}:{API_PORT}")
    print(f"ğŸ“– APIæ–‡æ¡£: http://{API_HOST}:{API_PORT}/docs")
    print(f"ğŸ”„ ReDocæ–‡æ¡£: http://{API_HOST}:{API_PORT}/redoc")

    uvicorn.run(
        "rag_api_server:app",
        host=API_HOST,
        port=API_PORT,
        reload=True,  # å¼€å‘æ¨¡å¼ä¸‹å¯ç”¨çƒ­é‡è½½
        log_level="info"
    )
